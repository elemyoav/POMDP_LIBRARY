ppo-dec-tiger:
    env: envs.tiger.tiger.DecTiger
    run: PPO
    stop:
        timesteps_total: 200000
    config:

        framework: torch
        lambda: 0.95
        gamma: 0.99
            # grid_search:
            #     - 0.9
            #     - 0.95
            #     - 0.99
        lr: 0.0001
            # grid_search:
            #     - 0.0001
            #     - 0.001
            #     - 0.01

        kl_coeff: 0.5
        clip_rewards: False
        clip_param: 0.1
        vf_clip_param: 10.0
        entropy_coeff: 0.01
        train_batch_size: 5000
        rollout_fragment_length: 100
        sgd_minibatch_size: 500
        num_sgd_iter: 10
        batch_mode: truncate_episodes
        observation_filter: NoFilter
        
        exploration_config:
          epsilon_timeout: 200000
          final_epsilon: 0.01
          
        model:
            vf_share_layers: true   
        multiagent:
          policies: ["pagent_0", "pagent_1"]
          # YAML-capable policy_mapping_fn definition via providing a callable class here.
          policy_mapping_fn:
            type: ray.rllib.examples.multi_agent_and_self_play.policy_mapping_fn.PolicyMappingFn